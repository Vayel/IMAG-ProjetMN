\documentclass[12pt]{article}

\usepackage{amsfonts, amsmath, amssymb, amstext, latexsym}
\usepackage{graphicx, epsfig}
\usepackage[latin1]{inputenc}
\usepackage[french]{babel}
\usepackage{exscale}
\usepackage{amsbsy}
\usepackage{amsopn}
\usepackage{fancyhdr}
\usepackage{amsmath,amssymb}
\usepackage{float}

\newcommand{\noi}{\noindent}
\newcommand{\dsp}{\displaystyle}
\newcommand{\ind}{{{\large 1} \hspace*{-1.6mm} {\large 1}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\ub}{\mathbf{u}}

\textheight 25cm
\textwidth 16cm
\oddsidemargin 0cm
\evensidemargin 0cm
\topmargin 0cm
\hoffset -0mm
\voffset -20mm

\pagestyle{plain}

\begin{document}

\section*{Question 1}

\[
	\frac{\partial ^2u}{\partial t^2}=\gamma ^2\frac{\partial ^2u}{\partial x^2}
\]
On a,
\[
    \frac{\partial ^2U(x)\cos(\omega t)}{\partial t^2}-\gamma^2\frac{\partial ^2U(x)\cos(\omega t)}{\partial x^2}=0
\]
\[
    U''(x)-\beta ^2U(x)=0,\quad\quad avec \beta ^2=\frac{\omega ^2}{\gamma ^2}
\]
Ce qui nous donne l'équation différentielle suivante:
\[
    U(x)=\lambda\cos(\beta x)+\mu\sin(\beta x),\quad \lambda,\mu \in \Re
\]
Par les conditions aux limites:
\[
    u(0,t)=u(L,t)=0
\]
\[
    U(0)=\lambda \cos(0)+\mu \sin(0)=\lambda+0 \quad \Longrightarrow \quad \lambda=0
\]
Donc: 
\[
    U(x)=\mu\sin(\beta x)
\]
Or,
\[
    U(L)=\mu\sin(\beta L)=0 \quad \Longrightarrow \quad \beta=\frac{n\pi}{L}
\]
On obtient bien:
\[
    U_n(x)=B_n\sin\left(n\pi \frac{x}{L}\right),\quad \omega_n=n\pi \frac{\gamma}{L}\Leftrightarrow f_n=n\frac{1}{2L}\sqrt{\frac{T}{\mu}}
\]
Par le principe de superpostion des ondes propres on a,
\[
    u(x,t)=\sum_{n=1}^{\infty}B_{n}sin(n\pi\frac{x}{L})cos(n\pi\frac{\gamma t}{L})
\]
Et par la condition aux limites:
\[
    u(x,t)=\sum_{n=1}^{\infty}B_{n}sin(n\pi\frac{x}{L})
\]
Avec Fourier on obtient que:
\[
    B_{n}=\frac{2}{L}\int_{0}^{L}u(x,t)sin(n\pi\frac{x}{L})dx
\]

Lorsque la longueur de la corde augmente, la fréquence fondamentale diminue. Lorsque la longueur est divisé par 2 on a $f_{n}=n\sqrt{\frac{T}{\mu}}$ et donc pour la fréquence fondamentale $f_{1}=\sqrt{\frac{T}{\mu}}$.
Lorsque la masse linéique aumente, la fréquence fondamentale diminue et lorsque la tension augment la fréquence fondamentale augmente également.

\section*{Question 2}

    A)
On a,
\[
    u(x+h,t)+u(x-h,t)=2u(x,t)+h^2\frac{\partial^2 u}{\partial x^2}(x,t)+o(h^2)
\]
\[
    u(x,t+k)+u(x,t-k)=2u(x,t)+k^2\frac{\partial^2 u}{\partial t^2}(x,t)+o(k^2)
\]
Ce qui nous donne:
\[
    -h^2\frac{\partial^2 u}{\partial x^2}(x,t)=2u(x,t)-u(x+h,t)-u(x-h,t)+o(h^2)
\]
\[
    -k^2\frac{\partial^2 u}{\partial t^2}(x,t)=2u(x,t)-u(x,t+k)-u(x,t-k)+o(k^2)
\]
Donc,
\[
    \frac{\partial^2 u}{\partial x^2}(x,t)=\frac{2u(x,t)-u(x+h,t)-u(x-h,t)}{h^2}+o(1)
\]
\[
    \frac{\partial^2 u}{\partial t^2}(x,t)=\frac{2u(x,t)-u(x,t+k)-u(x,t-k)}{k^2}+o(1)
\]
Par changement de notation on a bien:
\[
\begin{array}{l}
	\displaystyle \frac{\partial^2 u}{\partial x^2}(x_l,t_k)\approx \frac{1}{h^2}(u_{l+1}^n-2u_l^n+u_{l-1}^n)\\ \\ 		\displaystyle \frac{\partial^2 u}{\partial t^2}(x_l,t_k)\approx \frac{1}{k^2}(u_{l}^{n+1}-2u_l^n+u_{l}^{n-1})\end{array}
\]
    B)
\[
    u(x+2h,t)+u(x-2h,t)=2u(x,t)+4h^2\frac{\partial ^2u}{\partial x^2}(x,t)+\frac{4}{3}h^4\frac{\partial ^4u}{\partial x^4}(x,t)+o(h^4)
\]
\[
    -4[u(x+h,t)+u(x-h,t)]=-8u(x,t)-4h^2\frac{\partial ^2u}{\partial x^2}(x,t)-\frac{h^3}{3}\frac{\partial ^4u}{\partial x^4}(x,t)-o(h^4)
\]
Donc,
\[
    u(x+2h,t)+u(x-2h,t)-4[u(x+h,t)+u(x-h,t)]+6u(x,t)=h^4\frac{\partial ^4u}{\partial x^4}(x,t)
\]
Par simple changement de notation on a bien,
\[
\frac{\partial^4 u}{\partial x^4}(x_l,t_k)\approx \frac{1}{h^4}(u_{l+2}^n-4u_{l+1}^n+6u_l^n-4u_{l-1}^n+u_{l-2}^n)
\]
    C) On a
\[
    \frac{\partial u}{\partial t}(x+h,t)=\frac{u(x+h,t+k)-u(x+h,t-k)}{2k}+o(1)\approx \frac{u_{l+1}^{n+1}-u_{l+1}^{n-1}}{2k}
\]
\[
    \frac{\partial u}{\partial t}(x-h,t)=\frac{u(x-h,t+k)-u(x-h,t-k)}{2k}+o(1)\approx \frac{u_{l-1}^{n+1}-u_{l-1}^{n-1}}{2k}
\]
\[
    \frac{\partial 2u}{\partial t}(x,t)=\frac{2u(x,t+k)-2u(x,t-k)}{2k}+o(1) \approx \frac{2u_{l}^{n+1}-2u_{l}^{n-1}}{2k}
\]
D'où,
\[
\frac{\partial}{\partial t}\left(\frac{\partial^2 u}{\partial x^2}\right)\approx \frac{1}{2kh^2}(u_{l+1}^{n+1}-2u_{l}^{n+1}+u_{l-1}^{n+1}-u_{l+1}^{n-1}+2u_l^{n-1}-u_{l-1}^{n-1})
\]
    D) On réécris l'équation
\[
    \frac{\partial^2 u}{\partial t^2}-\gamma^2 \frac{\partial^2 u}{\partial x^2}+\kappa^2 \frac{\partial^4 u}{\partial x^4}+2\sigma_0 \frac{\partial u}{\partial t}-2\sigma_1 \frac{\partial^3 u}{\partial t\partial x^2}=0
\]
En développant les termes, on obtient:
\[
    -\frac{\sigma_{1}}{kh^2}u_{l-1}^{n+1}+(\frac{1}{k^2}+\frac{\sigma_{0}}{k}+\frac{2\sigma_{1}}{kh^2})u_l^{n+1}-\frac{\sigma_{1}}{kh^2}u_{l+1}^{n+1}
\]
\[
    +\quad\frac{\kappa^2}{h^4}u_{l-2}^n+(-\frac{\gamma ^2}{h^2}-\frac{4\kappa ^2}{h^4})u_{l-1}^n+(-\frac{2}{k^2}+\frac{2\gamma^2}{h^2}+\frac{6\kappa^2}{h^4})u_l^n+(-\frac{\gamma ^2}{h^2}-\frac{4\kappa ^2}{h^4})u_{l+1}^n+\frac{\kappa^2}{h^4}u_{l+2}^n
\]
\[
    +\quad\frac{\sigma_{1}}{kh^2}u_{l-1}^{n-1}+(\frac{1}{k^2}-\frac{\sigma_{0}}{k}-\frac{2\sigma_{1}}{kh^2})u_l^{n-1}+\frac{\sigma_{1}}{kh^2}u_{l+1}^{n-1}
\]
\[
    =0
\]
En multipliant l'équation par $k^2$ puis en remplacant les coefficients, on obtient le schéma implicite associé à l'EDP.

\section*{Question 3}

En considérant:
\[
    u(0,t)=\frac{\partial ^2u}{\partial x^2}(0,t)=\frac{1}{h^2}(u_{1}-2u_{0}+u_{-1})=0
\]
\[
    u(L,t)=\frac{\partial ^2u}{\partial x^2}(L,t)=\frac{1}{h^2}(u_{N+1}-2u_{N}+u_{N-1})=0
\]
On obtient que,
\[
    u_{1}=-u_{-1}+2u_{0}, \quad Or \quad u_{0}=0
\]
\[
    u_{N+1}=-u_{N-1}+2u_{N}, \quad Or \quad u_{N}=0
\]
D'où,
\[
    u_{1}=-u_{-1} \quad\quad\quad u_{N+1}=-u_{N-1}
\]

Si l'on considère i $\in$ [1;N], on a
\[
    \overline{\A}\overline{\ub}^{n+1}: a_{1}u_{i-1}^{n+1}+a_{2}u_{i}^{n+1}+a_{1}u_{i+1}^{n+1}
\]
\[
   \overline{\C}\overline{\ub}^{n-1}: c_{1}u_{i-1}^{n+1}+c_{2}u_{i}^{n+1}+c_{1}u_{i+1}^{n+1}
\]
Avec i $\in$ [2;N-1], on a 
\[
    \overline{\B}\overline{\ub}^n: b_{1}u_{i-2}^n+b_{2}u_{i-1}^n+b_{3}u_{i}^n+b_{2}u_{i+1}^n+b_{1}u_{i+2}^n
\]
Puis si $i=1$,
\[
    \overline{\B}\overline{\ub}^n: b_{2}u_{0}^n+b_{3}u_{1}^n-b_{1}u_{1}^n+b_{2}u_{2}^n+b_{1}u_{3}^n
\]
\[
    = b_{2}u_{0}^n+b_{3}u_{1}^n+b_{1}u_{-1}^n+b_{2}u_{2}^n+b_{1}u_{3}^n
\]
\[
    car -b_{1}u_{1}^n=b_{1}u_{-1}
\]
Et si $i=N$,
\[
    \overline{\B}\overline{\ub}^n: b_{1}u_{N-3}^n+b_{2}u_{N-2}^n-b_{1}u_{N-1}^n+b_{3}u_{N-1}^n+b_{2}u_{N}^n
\]
\[
    = b_{1}u_{N-3}^n+b_{2}u_{N-2}^n+b_{3}u_{N-1}^n+b_{2}u_{N}^n+b_{1}u_{N+1}^n
\]
\[
    car -b_{1}u_{N-1}^n=b_{1}u_{N+1}
\]
Enfin, pour $i=0$
\[
	\overline{\A} \overline{\ub}^{n+1}+\overline{\B} \overline{\ub}^n+\overline{\C} \overline{\ub}^{n-1}: u_{0}^{n+1}+u_{0}^{n}+u_{n-1}^{n-1}=0
\]
$i=N+1$
\[
	\overline{\A} \overline{\ub}^{n+1}+\overline{\B} \overline{\ub}^n+\overline{\C} \overline{\ub}^{n-1}: u_{N}^{n+1}+u_{N}^{n}+u_{N}^{n-1}=0
\]
La forme matricielle proposée permet donc de caractériser le shéma numérique.

\section*{Question 4}

    $\A=(1+\sigma_0 k)\mathbf{I}-\sigma_1 k \D_{xx}$
\[
    a_{i,i-1}=-\frac{\sigma_{1} k}{h^2}=a_{1}
\]
\[
    a_{i,i}=1+\sigma_{0}k+\frac{2\sigma_{1}k}{h^2}=a_{2}
\]
\[
    a_{i,i+1}=-\frac{\sigma_{1} k}{h^2}=a_{1}
\]
    $\C=(1-\sigma_0 k)\mathbf{I}+\sigma_1 k \D_{xx}$
\[
    c_{i,i-1}=\frac{\sigma_{1} k}{h^2}=c_{1}
\]
\[
    c_{i,i}=1-\sigma_{0}k-\frac{2\sigma_{1}k}{h^2}=c_{2}
\]
\[
    c_{i,i+1}=\frac{\sigma_{1} k}{h^2}=c_{1}
\]
\[
\D_{xxxx}=\frac{1}{h^4}
\begin{bmatrix}
    5 & -4 & 1 & & & \mathbf{0} \\
    -4 & 6 & -4 & 1 & & & \\
    1 & -4 & 6 & -4 & 1 & & \\ 
    &\ddots&\ddots&\ddots&\ddots&\ddots\\
    & & 1 & -4 & 6 & -4 \\
    \mathbf{0} & & & 1 & -4 & 5
\end{bmatrix}, 
\]
$\B=-2\mathbf{I}-\gamma^2 k^2 \D_{xx}+\kappa^2 k^2\D_{xxxx}$\\
\\Pour $i \in [1;N-2]$,
\[
    b_{i,i}=-2+\frac{2\gamma ^2 k^2}{h^2}+\frac{6\kappa^2 K^2}{h^4}
\]
Et avec $i \in [0;N-1]$
\[
    b_{i,i-2}=\frac{\kappa^2 k^2}{h^4}
\]
\[
    b_{i,i-1}=-\frac{\gamma^2 k^2}{h^2}-\frac{4\kappa^2 k^2}{h^4}
\]
\[
    b_{i,i+1}=-\frac{\gamma^2 k^2}{h^2}-\frac{4\kappa^2 k^2}{h^4}
\]
\[
    b_{i,i+2}=\frac{\kappa^2 k^2}{h^4}
\]
Enfin, pour $i=0$ et $i=N-1$
\[
    b_{i,i}=-2+\frac{2\gamma^2 k^2}{h^2}+\frac{5\kappa^2 k^2}{h^4}
\]
Or,
\[
    b_{3}-b_{1}=(-2+\frac{2\gamma ^2 k^2}{h^2}+\frac{6\kappa^2 K^2}{h^4})-\frac{\kappa^2 k^2}{h^4}=-2+\frac{2\gamma^2 k^2}{h^2}+\frac{5\kappa^2 k^2}{h^4}
\]
\Large{CQFD}\\

\section*{Question 5}

Par injection de $\tilde u(x,t)=e^{st+j\beta x}$
\[
    \frac{\partial ^2\tilde u}{\partial t^2}=s^2\tilde u \quad\quad \frac{\partial\tilde u}{\partial x^2}=-\beta^2 \tilde u
\]
\[
    \frac{\partial\tilde u}{\partial x^4}=\beta^4 \tilde u \quad\quad \frac{\partial\tilde u}{\partial t}=s\tilde u
\]
\[
    \frac{\partial^3 \tilde u}{\partial t\partial x^2}=-s\beta^2 \tilde u
\]
On a alors:
\[
    s^2=-\gamma^2 \beta^2 -\kappa^2 \beta^4 -2s\sigma_{0} -2s\beta^2 \sigma_{1}
\]
Ce qui nous donne l'équation de degré 2 suivante:
\[
    s^2 +2(\sigma_{0}+\beta^2 \sigma_{1})s +(\gamma^2\beta^2+\kappa^2\beta^4)=0
\]
Ainsi,
\[
    \Delta=4(\sigma_{0}+\beta^2\sigma_{1})^2-4(\gamma^2\beta^2+\kappa^2\beta^4)
\]
Or, puisque l'on a $\sigma_{0},\sigma_{1} \leqslant 0$ et petits, s possède deux racines complexes:
\[
    \frac{-2(\sigma_{0}+\beta^2 \sigma_{1})\pm2j\sqrt{\gamma^2 \beta^2+\kappa^2 \beta^4 -(\sigma_{0}+\beta^2 \sigma_{1})^2}}{2}
\]
\large{CQFD}\\\\
B) On considère l'expression suivante
\[
    \omega^2=\gamma^2\beta^2+\kappa^2\beta^4-\sigma_{0}^2-\beta^4\sigma_{1}^2-2\beta^2\sigma_{0}\sigma_{1}
\]
Ce qui nous donne une expression de $\beta^2$
\[
    (\kappa^2-\sigma_{1})(\beta^2)^2+(\gamma^2-2\sigma_{0}\sigma_{1})\beta^2-(\omega^2+\sigma_{0}^2)=0
\]
\[
    \Delta=(\gamma^2-2\sigma_{0}\sigma_{1})^2-4(\kappa^2-\sigma_{1})(-\omega^2-\sigma_{0}^2)
\]
$\beta^2$ admet donc deux racines réelles, on ne considère que la racine positive:
\[
\frac{-(\gamma^2-2\sigma_{0}\sigma_{1})+\sqrt{(\gamma^2-2\sigma_{0}\sigma_{1})^2+4(\omega^2+\sigma_{0}^2)(\kappa^2+\sigma_{1}^2)}}{2(\kappa^2-\sigma_{1}^2)}
 \]
 Et puisque $\sigma_{0}$ et $\sigma_{1}$ sont proches de 0, on obtient bien:
 \[
    \beta^2=\frac{-\gamma^2+\sqrt{\gamma^4+4\kappa^2 \omega^2}}{2\kappa^2}
 \]
 C) On considère,
 \[
     \sigma(\omega_{1})=-\sigma_{0}-\sigma_{1}\xi(\omega_{1})
 \]
 \[
     \sigma(\omega_{2})=-\sigma_{0}-\sigma_{1}\xi(\omega_{2})
 \]
 Ainsi,
 \[
     \sigma(\omega_{1})-\sigma(\omega_{2})=\sigma_{1}(\xi(\omega_{2})-\xi(\omega_{1}))
 \]
 Et,
 \[
     -\frac{6\ln{10}}{T_{60}(\omega_{1})}+\frac{6\ln{10}}{T_{60}(\omega_{2})}=\sigma_{1}(\xi(\omega_{2})-\xi(\omega_{1}))
 \]
 D'où,
 \[
     \frac{6\ln{10}}{\xi(\omega_{2})-\xi(\omega{1})}(-\frac{1}{T_{60}(\omega_{1})}+\frac{1}{T_{60}(\omega_{2})})=\sigma_{1}
 \]
 On a maintenant
 \[
     \sigma(\omega_{1})=-\sigma_{0}-\frac{6\ln{10}}{\xi(\omega_{2})-\xi(\omega_{1})}(-\frac{\xi(\omega_{1})}{T_{60}(\omega_{1})}+\frac{\xi(\omega_{1})}{T_{60}(\omega_{2})})
 \]
 Et
 \[
     -\frac{6\ln{10}}{T_{60}(\omega_{1})}=\sigma(\omega_{1})=-\sigma_{0}-\frac{6\ln{10}}{\xi(\omega_{2})-\xi(\omega_{1})}(-\frac{\xi(\omega_{1})}{T_{60}(\omega_{1})}+\frac{\xi(\omega_{1})}{T_{60}(\omega_{2})})
 \]
 Donc
 \[
     \sigma_{0}=-\frac{6\ln{10}}{\xi(\omega_{2})-\xi(\omega_{1})}(-\frac{\xi(\omega_{1})}{T_{60}(\omega_{1})}+\frac{\xi(\omega_{1})}{T_{60}(\omega_{2})}-\frac{\xi(\omega_{2})-\xi(\omega_{1})}{T_{60}(\omega_{1})})
 \]
 Ce qui nous donne finalement,
 \[
    \sigma_0=\frac{6 \ln 10}{\xi(\omega_2)-\xi(\omega_1)}\left(\frac{\xi(\omega_2)}{T_{60}(\omega_1)}-\frac{\xi(\omega_1)}{T_{60}(\omega_2)}\right),
\]

\section*{Question 6}
\begin{figure}
    \includegraphics[width=15cm]{vecteur_son_100000.png}
    \caption{Vecteur son pour 100000 iterations}
    \includegraphics[width=15cm]{vecteur_son_2000.jpg}
    \caption{Vecteur son pour 2000 itérations}
\end{figure}

\newpage
\section*{Question 7}

\large{Quantitativement}\\
\begin{figure}[!h]
    \includegraphics[width=15cm]{guitar_spectre.jpg}
    \caption{Spectre du vecteur out}
\end{figure}\\
\large{Qualitativement}\\\\
On voit que plus le paramètre d'inharmonicité est faible plus la timbre de la note est basse. Symétriquement, plus la valeur de B est élevée plus la timbre de la note est haute.\\
La position x0 ne semble pas vraiment impacter le timbre.\\


\section*{Question 8}

Le laplacien s'exprime en coordonnées polaires de la manière suivante :

$$
\Delta w = \frac{1}{r} \frac{\partial}{\partial r} (r \frac{\partial w}{\partial r}) +
\frac{1}{r^2} \frac{\partial^2 w}{\partial \theta^2} =
\frac{\partial^2 w}{\partial r^2} + \frac{1}{r} \frac{\partial w}{\partial r} + \frac{1}{r^2} \frac{\partial^2 w}{\partial \theta^2}
$$

En effet :

$$
\begin{aligned}
  \frac{\partial^2 w}{\partial x^2} + \frac{\partial^2 w}{\partial y^2} &=
  \frac{\partial}{\partial x} (\frac{\partial w}{\partial r} \frac{\partial r}{\partial x} + \frac{\partial w}{\partial \theta} \frac{\partial \theta}{\partial x}) + \frac{\partial}{\partial y} (\frac{\partial w}{\partial r} \frac{\partial r}{\partial y} + \frac{\partial w}{\partial \theta} \frac{\partial \theta}{\partial y}) \\
  &= \frac{\partial}{\partial x} (\frac{\partial w}{\partial r} \frac{x}{\sqrt{x^2 + y^2}} + \frac{\partial w}{\partial \theta} \frac{-y}{x^2 + y^2}) + \frac{\partial}{\partial y} (\frac{\partial w}{\partial r} \frac{y}{\sqrt{x^2 + y^2}} + \frac{\partial w}{\partial \theta} \frac{x}{x^2 + y^2})
\end{aligned}
$$

Or :

$$
\left\{\begin{aligned}
    \frac{\partial w}{\partial r} &= \frac{1}{a} \frac{\partial w}{\partial \eta} \\
    \frac{\partial^2 w}{\partial r^2} &= \frac{1}{a^2} \frac{\partial^2 w}{\partial \eta^2} \\
    \frac{\partial^2 w}{\partial t^2} &= \frac{c^2}{a^2} \frac{\partial^2 w}{\partial \tau^2} \\
                                      &= \frac{T}{\rho a^2} \frac{\partial^2 w}{\partial \tau^2}
\end{aligned}\right.
$$

Donc :

$$
\begin{aligned}
  \rho \dfrac{\partial^2 w}{\partial t^2} = T \Delta w
  &\Leftrightarrow
  \rho \frac{T}{\rho a^2} \frac{\partial^2 w}{\partial \tau^2} = T(\frac{1}{a^2} \frac{\partial^2 w}{\partial \eta^2} + \frac{1}{a\eta} \frac{1}{a} \frac{\partial w}{\partial \eta} + \frac{1}{\eta^2 a^2} \frac{\partial^2 w}{\partial \theta^2}) \\
  &\Leftrightarrow
  \frac{\partial^2 w}{\partial \tau^2} = \frac{\partial^2 w}{\partial \eta^2} + \frac{1}{\eta} \frac{\partial w}{\partial \eta} + \frac{1}{\eta^2} \frac{\partial^2 w}{\partial \theta^2}
\end{aligned}
$$

\section*{Question 9}

En $\eta = 0$, le laplacien diverge donc la discrétisation aussi. On se place
donc dans le cas $i > 1$.

$w$ est deux fois dérivable par rapport à chacun de ses paramètres. On peut donc
lui appliquer un développement de Taylor :

$$
\left\{\begin{aligned}
  w(\eta_{i+1}, \theta_j, \tau_n) &= w(\eta_i + d\eta, \theta_j, \tau_n) \\
                                  &= w(\eta_i, \theta_j, \tau_n) + d\eta \frac{\partial w}{\partial \eta}(\eta_i, \theta_j, \tau_n) + \frac{d\eta^2}{2} \frac{\partial^2 w}{\partial \eta^2}(\eta_i, \theta_j, \tau_n) + o(d\eta^2) \ (1) \\
  w(\eta_{i-1}, \theta_j, \tau_n) &= w(\eta_i - d\eta, \theta_j, \tau_n) \\
                                  &= w(\eta_i, \theta_j, \tau_n) - d\eta \frac{\partial w}{\partial \eta}(\eta_i, \theta_j, \tau_n) + \frac{d\eta^2}{2} \frac{\partial^2 w}{\partial \eta^2}(\eta_i, \theta_j, \tau_n) + o(d\eta^2) \ (2)
\end{aligned}\right.
$$

Donc :

$$
\left\{\begin{aligned}
    \frac{w(\eta_{i+1}, \theta_j, \tau_n) - w(\eta_{i-1}, \theta_j, \tau_n)}{2d\eta} &= \frac{\partial w}{\partial \eta}(\eta_i, \theta_j, \tau_n) + o(d\eta) \ (1) - (2) \\
    \frac{w(\eta_{i+1}, \theta_j, \tau_n) - 2w(\eta_i, \theta_j, \tau_n) + w(\eta_{i-1}, \theta_j, \tau_n)}{d\eta^2} &= \frac{\partial^2 w}{\partial \eta^2}(\eta_i, \theta_j, \tau_n) + o(1) \ (1) + (2)
\end{aligned}\right.
$$

On a un résultat analogue pour les dérivées par rapport à $\theta$ et $\tau$, donc on en déduit l'équation discrétisée.

\section*{Question 10}

\begin{tabular}{|c|c|}
  \hline
  Equation & Discrétisation \\
  \hline
  $w(1, \theta, \tau) = 0$ & $w_{1, j}^n = 0$ \\
  \hline
  $w(\eta,\theta,0) = w_0(\eta \cos(\theta),\eta \sin (\theta))$ & $w_{i, j}^0 = w_0(\eta_i \cos(\theta_j),\eta_i \sin (\theta_j))$ \\
  \hline
  $\dfrac{\partial w(\eta, \theta, 0)}{\partial \tau} = 0$ & $\dfrac{w_{i, j}^1 - w_{i, j}^{-1}}{2d\tau} = 0$ \\
  \hline
\end{tabular}

\section*{Question 11}

En coordonnées cartésiennes, le laplacien est défini de partout, donc on peut
exprimer le schéma explicite pour tout indice :

$$
\rho \frac{W_{k, l}^{m+1} - 2W_{k, l}^m + W_{k, l}^{m-1}}{dt^2} =
T \times (\frac{W_{k+1, l}^m - 2W_{k, l}^m + W_{k-1, l}^m}{dx^2} + \frac{W_{k, l+1}^m - 2W_{k, l}^m + W_{k, l-1}^m}{dy^2})
$$

On l'étudie en $(k, l) = (1, 1)$ pour avoir $\eta = 0$ :

$$
\rho \frac{W_{1, 1}^{m+1} - 2W_{1, 1}^m + W_{1, 1}^{m-1}}{dt^2} =
T \times (\frac{W_{2, 1}^m - 2W_{1, 1}^m + W_{0, 1}^m}{dx^2} + \frac{W_{1, 2}^m - 2W_{1, 1}^m + W_{1, 0}^m}{dy^2})
$$

Pour $N_{\theta} = 8$, en prenant $dx = dy = dr = d\eta * a$, on a :

$$
\left\{\begin{aligned}
W_{1, 1} &= w_{1, 1} \\
W_{2, 1} &= w_{2, 1} \\
W_{0, 1} &= w_{2, 5} \\
W_{1, 2} &= w_{2, 3} \\
W_{1, 0} &= w_{2, 7} \\
\end{aligned}\right.
$$

Comme $d\tau^2 = \frac{c^2}{a^2}dt^2$, on en déduit la première équation.
La deuxième se retrouve en considérant un repère cartésien $(O, x', y')$
rotation de $45°$ du repère $(O, x, y)$.

En sommant les deux équations, on obtient l'égalité de la condition limite en
$\eta = 0$ pour $N_{\theta} = 8$.

Ce résultat se généralise facilement pour $N_{\theta} = 4n$, puisqu'il suffit
de considérer les repères cartésiens pivotés d'un angle $i * \frac{90}{n}$ degrés, $0 \leq i \leq n-1$.

\section*{Question 12}



\section*{Question 13}

Avec un développement de Taylor à l'ordre 4, on obtient :

$$
\begin{aligned}
    w_{i, j}^{n+1} + w_{i, j}^{n-1} &= 2w_{i, j}^n + d\tau^2 \frac{\partial^2 w}{\partial \tau^2}(\eta_i, \theta_j, \tau_n)
                                                                    + \frac{d\tau^4}{12} \frac{\partial^4 w}{\partial \tau^4}(\eta_i, \theta_j, \tau_n) + o(d\tau^4)
\end{aligned}
$$

Donc :

$$
\begin{aligned}
    \frac{w_{i, j}^{n+1} - 2w_{i, j}^n + w_{i, j}^{n-1}}{d\tau^2} &= \frac{\partial^2 w}{\partial \tau^2}(\eta_i, \theta_j, \tau_n)
                                                                     + \frac{d\tau^2}{12} \frac{\partial^4 w}{\partial \tau^4}(\eta_i, \theta_j, \tau_n) + o(d\tau^2)
\end{aligned}
$$

Or $\frac{\partial^4 w}{\partial \tau^4}$ est continue donc bornée sur un compact borné. Donc $\frac{\partial^4 w}{\partial \tau^4} = \mathcal{O}(d\tau^2)$

Donc le schéma est d'ordre 2 en temps.

De plus, on obtient le même résultat pour les dérivées secondes en espace.
Pour la dérivée première en $\eta$, on développe jusqu'à l'ordre 3, ce qui donne un ordre 2
une fois qu'on divise par $d\eta$.

\section*{Question 14}

On a donc $\omega(\eta,\theta,\tau)=F(\eta,\theta)cos(\omega\tau)$. On commence par remplacer  $\omega(\eta,\theta,\tau)$ dans l'EDP.
\[
    -F(\eta,\theta)\omega^2cos(\omega\tau)=\frac{\partial^2F(\eta,\theta)}{\partial \eta^2}cos(\omega\tau)+\frac{1}{\eta^2}\frac{\partial^2F(\eta,\theta)}{\partial \theta^2}cos(\omega\tau)+\frac{1}{\eta}\frac{\partial F(\eta,\theta)}{\partial\eta}cos(\omega\tau)
\]
En simplifiant on a:
\[
    -\omega^2F(\eta,\theta)=\frac{\partial^2F(\eta,\theta)}{\partial \eta^2}+\frac{1}{\eta^2}\frac{\partial^2F(\eta,\theta)}{\partial \theta^2}+\frac{1}{\eta}\frac{\partial F(\eta,\theta)}{\partial\eta}
\]
On injecte alors dans l'équation $F(\eta,\theta)=\sum_{n}F_{n}(\eta)cos(n\theta)$ et on divise par $cos(n\theta)$
\[
    -\omega^2\sum_{n}F_{n}(\eta)=\sum_{n}\frac{d^2F_{n}(\eta)}{d\eta^2}-\frac{n^2}{\eta^2}\sum_{n}F_{n}(\eta)+\frac{1}{\eta}\frac{dF_{n}(\eta)}{d\eta}
\]
En divisant par $\omega^2$ on obtient,
\[
    \sum_{n}F_{n}(\eta)\frac{d^2F_{n}(\eta)}{d\eta^2\omega^2}-\frac{n^2}{\omega^2\eta^2}F_{n}(\eta)+\frac{1}{\omega\eta}\frac{dF_{n}(\eta)}{d\eta\omega}=0
\]
On obtient bien l'équation de Bessel suivante:
\[
    \sum_{n}\frac{d^2F_{n}}{d\alpha^2}+\frac{1}{\omega\eta}\frac{dF_{n}}{d\alpha}+(1-\frac{n^2}{\alpha^2})F_{n}=0
\]
\section*{Question 15}

Se référer au fichier \verb+animation15.gif+.

\section*{Question 16}

L'animation de la solution exacte est dans le fichier \verb+animation16.gif+.

Les erreurs relatives ont été calculées selon la formule :

$$
\frac{w_{num} - w_{ex}}{|w_{ex}|}
$$

\begin{figure}[H]
  \includegraphics[scale=0.25]{q16relerror.png}
\end{figure}

L'erreur relative décroit avec le CFL, ce qui coïncide avec le fait que le schéma
est consistant en temps.

\section*{Question 17}

\begin{figure}[H]
  \includegraphics[scale=0.25]{q17relerror.png}
\end{figure}

La valeur du CFL n'influe pas sur l'erreur relative, laquelle augmente : le
schéma n'est pas stable.

\section*{Question 18}

Dans la question 16, l'erreur dépendait uniquement de $\eta$ et $\tau$ et elle
décroissait avec le CFL. En 17, où on introduit un terme en $\theta$, on remarque
que l'erreur ne diminue pas.

On en déduit que l'instabilité provient du terme en $\theta$, même si le changement
de formule peut également avoir une influence.

\section*{Question 19}

\begin{figure}[H]
  \includegraphics[scale=0.4]{q19_relerror.pdf}
\end{figure}

On lit sur le schéma que lorsque $N_{\theta}$ et $N_{\eta}$ sont multipliés par deux,
l'erreur l'est par quatre. Quand ils sont multipliés par quatre, l'erreur l'est par seize.

Il aurait été judicieux de tracer le rapport des erreurs relatives pour deux grilles.

\section*{Question 20}

La simulation donne des résultats étranges, différents de l'animation fournie.

\end{document}
